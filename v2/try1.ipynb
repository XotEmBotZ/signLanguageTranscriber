{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "from pprint import pprint\n",
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "BaseOptions = mp.tasks.BaseOptions\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "HandLandmarker = mp.tasks.vision.HandLandmarker\n",
    "HandLandmarkerOptions = mp.tasks.vision.HandLandmarkerOptions\n",
    "\n",
    "FaceLandmarker = mp.tasks.vision.FaceLandmarker\n",
    "FaceLandmarkerOptions = mp.tasks.vision.FaceLandmarkerOptions\n",
    "\n",
    "\n",
    "PoseLandmarker = mp.tasks.vision.PoseLandmarker\n",
    "PoseLandmarkerOptions = mp.tasks.vision.PoseLandmarkerOptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "hand_model_path = './models/hand_landmarkr_full.task'\n",
    "face_model_path = './models/face_landmarker.task'\n",
    "pose_model_path = './models/pose_landmarker_heavy.task'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "handOptions = HandLandmarkerOptions(\n",
    "    base_options=BaseOptions(model_asset_path=hand_model_path),\n",
    "    running_mode=VisionRunningMode.VIDEO,\n",
    "    num_hands=2)\n",
    "\n",
    "faceOptions = FaceLandmarkerOptions(\n",
    "    base_options=BaseOptions(model_asset_path=face_model_path),\n",
    "    running_mode=VisionRunningMode.VIDEO)\n",
    "\n",
    "poseOptions = PoseLandmarkerOptions(\n",
    "    base_options=BaseOptions(model_asset_path=pose_model_path),\n",
    "    running_mode=VisionRunningMode.VIDEO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [],
   "source": [
    "def initModels():\n",
    "    handLandmarker=HandLandmarker.create_from_options(handOptions)\n",
    "    faceLandmarker=FaceLandmarker.create_from_options(faceOptions)\n",
    "    poseLandmarker=PoseLandmarker.create_from_options(poseOptions)\n",
    "    return handLandmarker,faceLandmarker,poseLandmarker\n",
    "handLandmarker=HandLandmarker.create_from_options(handOptions)\n",
    "faceLandmarker=FaceLandmarker.create_from_options(faceOptions)\n",
    "poseLandmarker=PoseLandmarker.create_from_options(poseOptions)\n",
    "# initModels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def draw_landmarks_on_image(rgb_image, pose_detection_result,hand_detection_result):\n",
    "  pose_landmarks_list = pose_detection_result.pose_landmarks\n",
    "  \n",
    "  hand_landmarks_list = hand_detection_result.hand_landmarks\n",
    "  handedness_list = hand_detection_result.handedness\n",
    "  \n",
    "  annotated_image = np.copy(rgb_image)\n",
    "\n",
    "  MARGIN = 10  # pixels\n",
    "  FONT_SIZE = 1\n",
    "  FONT_THICKNESS = 1\n",
    "  HANDEDNESS_TEXT_COLOR = (88, 205, 54) # vibrant green\n",
    "\n",
    "  # Loop through the detected poses to visualize.\n",
    "  for idx in range(len(pose_landmarks_list)):\n",
    "    pose_landmarks = pose_landmarks_list[idx]\n",
    "\n",
    "    # Draw the pose landmarks.\n",
    "    pose_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "    pose_landmarks_proto.landmark.extend([\n",
    "        landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in pose_landmarks\n",
    "    ])\n",
    "    solutions.drawing_utils.draw_landmarks(\n",
    "        annotated_image,\n",
    "        pose_landmarks_proto,\n",
    "        solutions.pose.POSE_CONNECTIONS,\n",
    "        solutions.drawing_styles.get_default_pose_landmarks_style())\n",
    "    \n",
    "  # Loop through the detected hands to visualize.\n",
    "  for idx in range(len(hand_landmarks_list)):\n",
    "    hand_landmarks = hand_landmarks_list[idx]\n",
    "    handedness = handedness_list[idx]\n",
    "\n",
    "    # Draw the hand landmarks.\n",
    "    hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "    hand_landmarks_proto.landmark.extend([\n",
    "        landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in hand_landmarks\n",
    "    ])\n",
    "    solutions.drawing_utils.draw_landmarks(\n",
    "        annotated_image,\n",
    "        hand_landmarks_proto,\n",
    "        solutions.hands.HAND_CONNECTIONS,\n",
    "        solutions.drawing_styles.get_default_hand_landmarks_style(),\n",
    "        solutions.drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "    # Get the top left corner of the detected hand's bounding box.\n",
    "    height, width, _ = annotated_image.shape\n",
    "    x_coordinates = [landmark.x for landmark in hand_landmarks]\n",
    "    y_coordinates = [landmark.y for landmark in hand_landmarks]\n",
    "    text_x = int(min(x_coordinates) * width)\n",
    "    text_y = int(min(y_coordinates) * height) - MARGIN\n",
    "\n",
    "    # Draw handedness (left or right hand) on the image.\n",
    "    cv2.putText(annotated_image, f\"{handedness[0].category_name}\",\n",
    "                (text_x, text_y), cv2.FONT_HERSHEY_DUPLEX,\n",
    "                FONT_SIZE, HANDEDNESS_TEXT_COLOR, FONT_THICKNESS, cv2.LINE_AA)\n",
    "  return annotated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def save_data(pose_landmarket_result, hand_landmarker_result, name=None,npReturn=False):\n",
    "    \n",
    "    if len(pose_landmarket_result.pose_landmarks) > 0:\n",
    "        finalPoseData: np.ndarray = np.array([[i.x, i.y, i.z]\n",
    "                                              for i in pose_landmarket_result.pose_landmarks[0]]).flatten()\n",
    "    else:\n",
    "        finalPoseData = np.zeros((99,))\n",
    "    finalLeftHandData = np.zeros((63,))\n",
    "    finalRightHandData = np.zeros((63,))\n",
    "    for idx in range(len(hand_landmarker_result.hand_landmarks)):\n",
    "        \n",
    "        if hand_landmarker_result.handedness[idx][0].category_name == \"Left\":\n",
    "            finalLeftHandData=np.array([[i.x, i.y, i.z] for i in hand_landmarker_result.hand_landmarks[[idx][0]]]).flatten()\n",
    "        else:\n",
    "            finalRightHandData = np.array([[i.x, i.y, i.z] for i in hand_landmarker_result.hand_landmarks[[idx][0]]]).flatten()\n",
    "\n",
    "    if npReturn:\n",
    "        return np.concatenate([ finalPoseData, finalLeftHandData, finalRightHandData])\n",
    "    np.save(name, np.concatenate([ finalPoseData, finalLeftHandData, finalRightHandData]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "prev_frame_time = 0\n",
    "new_frame_time = 0\n",
    "startTime=time.time()\n",
    "handLandmarker, faceLandmarker, poseLandmarker = initModels()\n",
    "while cap.isOpened():\n",
    "    new_frame_time = time.time()\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB,\n",
    "                        data=cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "    hand_landmarker_result = handLandmarker.detect_for_video(\n",
    "        mp_image,int((time.time()-startTime)*1000))\n",
    "    pose_landmarket_result = poseLandmarker.detect_for_video(\n",
    "        mp_image,int((time.time()-startTime)*1000))\n",
    "\n",
    "    image = cv2.cvtColor(draw_landmarks_on_image(\n",
    "        mp_image.numpy_view(), pose_landmarket_result,hand_landmarker_result), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # pprint(hand_landmarker_result.handedness)\n",
    "    save_data(pose_landmarket_result, hand_landmarker_result, \"testSave.npy\")\n",
    "\n",
    "    fps = 1/(new_frame_time-prev_frame_time)\n",
    "    prev_frame_time = new_frame_time\n",
    "    cv2.putText(image, f\"{fps:1f}\", (15, 50),\n",
    "                cv2.FONT_HERSHEY_PLAIN, 2, (255, 255, 255), 3)\n",
    "    cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sign=['control','yes','no','thankYou','hello','iLoveYou','peace','please',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join('data')\n",
    "actions = np.array(sign)\n",
    "no_sequences = 100\n",
    "# sequence_length = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for action in actions:\n",
    "    try:\n",
    "        os.mkdir(DATA_PATH+f'/{action}')\n",
    "    except:\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collectData(action, length=no_sequences,start=0):\n",
    "    handLandmarker, faceLandmarker, poseLandmarker = initModels()\n",
    "    startTime = time.time()\n",
    "    for frame_num in range(length):\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB,\n",
    "                            data=cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        hand_landmarker_result = handLandmarker.detect_for_video(\n",
    "            mp_image, int((time.time()-startTime)*1000))\n",
    "        pose_landmarket_result = poseLandmarker.detect_for_video(\n",
    "            mp_image, int((time.time()-startTime)*1000))\n",
    "\n",
    "        image = cv2.cvtColor(draw_landmarks_on_image(\n",
    "            mp_image.numpy_view(), pose_landmarket_result, hand_landmarker_result), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        if frame_num == 0:\n",
    "            cv2.putText(image, 'STARTING COLLECTION', (120, 200),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 4, cv2.LINE_AA)\n",
    "            cv2.putText(image, 'Collecting frames for {} Frame No {}'.format(action, frame_num), (15, 12),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "            # Show to screen\n",
    "            cv2.imshow('OpenCV Feed', image)\n",
    "            cv2.waitKey(500)\n",
    "        else:\n",
    "            cv2.putText(image, 'Collecting frames for {} Frame No {}'.format(action, frame_num), (15, 12),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "            # Show to screen\n",
    "            cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        npy_path = os.path.join(\n",
    "            DATA_PATH,action, str(start+frame_num))\n",
    "        save_data(pose_landmarket_result,\n",
    "                    hand_landmarker_result, npy_path, )\n",
    "        # Break gracefully\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "collectData('peace',500,0)\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sign)\n",
    "cap = cv2.VideoCapture(0)\n",
    "for action in actions:\n",
    "    input()\n",
    "    collectData(action,500,0)\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label: num for num, label in enumerate(actions)}.copy()\n",
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences, labels = [], []\n",
    "for action in actions:\n",
    "    \n",
    "    for frame_num in range(500):\n",
    "        res = np.load(os.path.join(DATA_PATH, action, str(frame_num)+'.npy',))\n",
    "        \n",
    "        sequences.append(res)\n",
    "        labels.append(label_map[action])\n",
    "np.array(sequences).shape, np.array(labels).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(sequences)\n",
    "Y = to_categorical(labels).astype(int)\n",
    "X.shape,Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape[1:],actions.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# model.add(LSTM(64, return_sequences=True,\n",
    "#           activation='relu', input_shape=X_train.shape[1:]))\n",
    "# model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "# model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "# model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(256, activation='relu' , input_shape=X_train.shape[1:]))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy',\n",
    "              metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train, epochs=2000, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.predict(X_test)\n",
    "actions[np.argmax(res[4])], actions[np.argmax(Y_test[4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(X_test)\n",
    "ytrue = np.argmax(Y_test, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()\n",
    "multilabel_confusion_matrix(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.expand_dims(X_test[0], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [(245, 117, 16), (117, 245, 16), (16, 117, 245)]\n",
    "\n",
    "\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0, 60+num*40),\n",
    "                      (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    prev_frame_time = 0\n",
    "    new_frame_time = 0\n",
    "    startTime = time.time()\n",
    "    handLandmarker, faceLandmarker, poseLandmarker = initModels()\n",
    "    while cap.isOpened():\n",
    "        new_frame_time = time.time()\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB,\n",
    "                            data=cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        hand_landmarker_result = handLandmarker.detect_for_video(\n",
    "            mp_image, int((time.time()-startTime)*1000))\n",
    "        pose_landmarket_result = poseLandmarker.detect_for_video(\n",
    "            mp_image, int((time.time()-startTime)*1000))\n",
    "\n",
    "        image = cv2.cvtColor(draw_landmarks_on_image(\n",
    "            mp_image.numpy_view(), pose_landmarket_result, hand_landmarker_result), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # pprint(hand_landmarker_result.handedness)\n",
    "        concatArray=save_data( pose_landmarket_result,\n",
    "                hand_landmarker_result, npReturn=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        predict = model.predict(np.expand_dims(concatArray, axis=0))\n",
    "        predictF=actions[np.argmax(predict)]\n",
    "        cv2.putText(image, f\"{predictF}\", (15, 50),\n",
    "                    cv2.FONT_HERSHEY_PLAIN, 2, (255, 255, 255), 3)\n",
    "        cv2.putText(image, f\"{(np.max(predict))}\", (15, 410),\n",
    "                    cv2.FONT_HERSHEY_PLAIN, 2, (255, 255, 255), 3)\n",
    "        # cv2.putText(image, f\"{predict[0][1]}\", (15, 440),\n",
    "        #             cv2.FONT_HERSHEY_PLAIN, 2, (255, 255, 255), 3)\n",
    "        # cv2.putText(image, f\"{predict[0][2]}\", (15, 470),\n",
    "        #             cv2.FONT_HERSHEY_PLAIN, 2, (255, 255, 255), 3)\n",
    "\n",
    "        \n",
    "\n",
    "        fps = 1/(new_frame_time-prev_frame_time)\n",
    "        prev_frame_time = new_frame_time\n",
    "        # cv2.putText(image, f\"{fps:1f}\", (15, 50),\n",
    "        #             cv2.FONT_HERSHEY_PLAIN, 2, (255, 255, 255), 3)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "except Exception as e:\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: modelv2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: modelv2/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"modelv2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['control', 'yes', 'no', 'thankYou', 'hello', 'iLoveYou', 'peace',\n",
       "       'please'], dtype='<U8')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
